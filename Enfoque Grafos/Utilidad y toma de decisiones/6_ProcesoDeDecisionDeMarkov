# Proceso de Decisi√≥n de Markov (MDP)
class MDP:
    def __init__(self, estados, acciones, recompensas, transiciones, gamma=0.9):
        self.estados = estados
        self.acciones = acciones
        self.recompensas = recompensas
        self.transiciones = transiciones
        self.gamma = gamma

    def valor_estado(self, estado):
        return max(sum(self.transiciones[(estado, accion)][sp] * (self.recompensas[(estado, accion, sp)] + self.gamma * self.valor_estado(sp)) for sp in self.estados) for accion in self.acciones)

# Ejemplo de uso
estados = ['A', 'B']
acciones = ['1', '2']
recompensas = {('A', '1', 'B'): 10, ('A', '2', 'B'): 5, ('B', '1', 'A'): -1, ('B', '2', 'A'): 0}
transiciones = {('A', '1'): {'B': 1.0}, ('A', '2'): {'B': 1.0}, ('B', '1'): {'A': 1.0}, ('B', '2'): {'A': 1.0}}
mdp = MDP(estados, acciones, recompensas, transiciones)
print(f"Valor del estado A: {mdp.valor_estado('A')}")
